{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen = capacity)\n",
    "\n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN2(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(DQN2, self).__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(1, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "\n",
    "        nn.Linear(16, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "\n",
    "        nn.Linear(16, 10),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.net(x).to(device)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_# 연료와 스테이트 개수 정의\n",
    "fuel = 299\n",
    "goal = 63\n",
    "# 중력\n",
    "g = 9.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각도 쎄타, v의 속도로 발사할 때 가는 거리\n",
    "def distance(v, theta, wind_v):\n",
    "  return (v*np.cos(theta) - wind_v) * 2 * v * np.sin(theta) / g\n",
    "\n",
    "# 걸리는 시간\n",
    "def time_cost(v, theta):\n",
    "  return v * np.sin(theta) / g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 액션별 바람 저항\n",
    "def state_wind(v):\n",
    "  if v == 5 or v == 6:\n",
    "    wind_v = 0\n",
    "  \n",
    "  elif v == 7 or v == 8:\n",
    "    wind_v = 1\n",
    "  \n",
    "  elif v == 9 or v == 10:\n",
    "    wind_v = 2\n",
    "  \n",
    "  elif v == -5 or v == -6:\n",
    "    wind_v = 0\n",
    "  \n",
    "  elif v == -7 or v == -8:\n",
    "    wind_v = -1\n",
    "\n",
    "  elif v == -9 or v == -10:\n",
    "    wind_v = -2\n",
    "  \n",
    "  return wind_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종료 계산\n",
    "def is_terminate(fuel, next_state):\n",
    "  if fuel < 0:\n",
    "    print('lose')\n",
    "    return True\n",
    "\n",
    "  elif next_state > goal:\n",
    "    print('lose')\n",
    "    return True\n",
    "  \n",
    "  elif goal - 4 < next_state <= goal:\n",
    "    print('win')\n",
    "    return True\n",
    "  \n",
    "  else:\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(fuel, next_state):\n",
    "    \n",
    "  if fuel < 0 or next_state > goal :\n",
    "    reward = -100\n",
    "  \n",
    "  elif goal - 1 < next_state <= goal:\n",
    "    reward = 100\n",
    "  \n",
    "  else:\n",
    "    reward = -1\n",
    "  \n",
    "  return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = {'action1': (5, np.pi/6),\n",
    "          'action2': (5, np.pi/3),\n",
    "          'action3': (6, np.pi/6),\n",
    "          'action4': (6, np.pi/3),\n",
    "          'action5': (7, np.pi/6),\n",
    "          'action6': (7, np.pi/3),\n",
    "          'action7': (8, np.pi/6),\n",
    "          'action8': (8, np.pi/3),\n",
    "          'action9': (9, np.pi/6),\n",
    "          'action10': (9, np.pi/3), \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_selection = ['action1','action2','action3','action4','action5','action6', 'action7', 'action8', 'action9', 'action10']\n",
    "num_action = 10\n",
    "\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "\n",
    "target_update = 3\n",
    "\n",
    "n_actions = 10\n",
    "\n",
    "current_s = torch.Tensor([0])\n",
    "\n",
    "policy_net = DQN2().to(device)\n",
    "target_net = DQN2().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.SGD(policy_net.parameters(), lr = 1e-3)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "  with torch.no_grad():\n",
    "    return policy_net(state).max(1)[1].view(1, 1)    \n",
    "\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "  plt.figure(2)\n",
    "  plt.clf()\n",
    "  durations_t = torch.tensor(episode_durations, dtype = torch.float)\n",
    "  plt.title('Training...')\n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Duration')\n",
    "  plt.plot(durations_t.numpy())\n",
    "  if len(durations_t) >= 100:\n",
    "    means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "    means = torch.cat((torch.zeros(99), means))\n",
    "    plt.plot(means.numpy())\n",
    "\n",
    "  plt.pause(0.001) \n",
    "  if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())  \n",
    "        \n",
    "# env 정의, (next_state, reward, done, {})을 반환\n",
    "def env(current_location, current_action, fuel):\n",
    "  v, theta = action[current_action]\n",
    "  wind_v = state_wind(v)\n",
    "  c_distance = distance(v, theta, wind_v)\n",
    "  next_location = current_location + c_distance\n",
    "  next_state = next_location\n",
    "  c_reward = reward(fuel, next_state)\n",
    "  done = is_terminate(fuel, next_state)\n",
    "  environment = (next_state, c_reward, done, {})\n",
    "  return environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_model():\n",
    "  if len(memory) < batch_size:\n",
    "    return  \n",
    "  transitions = memory.sample(batch_size)\n",
    "  batch = Transition(*zip(*transitions))\n",
    "\n",
    "  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device = device, dtype = torch.bool)\n",
    "  non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
    "  state_batch = torch.cat(batch.state).to(device)\n",
    "  action_batch = torch.cat(batch.action)\n",
    "  action_batch_long = torch.tensor(action_batch, dtype = torch.int64).to(device)\n",
    "  reward_batch = torch.cat(batch.reward)\n",
    "  \n",
    "  state_action_values = policy_net(state_batch).gather(1, action_batch_long)\n",
    "\n",
    "  next_state_values = torch.zeros(batch_size, device = device)\n",
    "  next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "  expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "  total_loss.append(loss)\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  for param in policy_net.parameters():\n",
    "    param.grad.data.clamp_(-1, 1)\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "total_loss = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "  location = 0\n",
    "  old_state = torch.tensor([[0]])\n",
    "  next_state_train = 0\n",
    "  fuel = 299  \n",
    "  for t in count():\n",
    "    action_idx = int(select_action(torch.Tensor([[next_state_train]]).to(device)).item()) # ex) 5\n",
    "    action_type = action_selection[action_idx] # ex) 'action4'\n",
    "    v, theta = action[action_type]\n",
    "    next_state_train, reward_c, done, _ = env(location, action_type, fuel)\n",
    "    next_state_train = torch.Tensor([[next_state_train]])\n",
    "    wind_v = state_wind(v)\n",
    "    location = location + distance(v, theta, wind_v)\n",
    "    reward_c = torch.tensor([[reward_c]], device = device)\n",
    "\n",
    "    if not done:\n",
    "      next_state_train = next_state_train\n",
    "    \n",
    "    else:\n",
    "      next_state_train = None\n",
    "\n",
    "    print(next_state_train)\n",
    "    action_idx_t = torch.Tensor([[action_idx]])\n",
    "    memory.push(old_state, action_idx_t, next_state_train, reward_c)\n",
    "\n",
    "    old_state = next_state_train\n",
    "\n",
    "    optimizer_model()\n",
    "    if done:\n",
    "      episode_durations.append(t + 1)\n",
    "      plot_durations()\n",
    "      break\n",
    "\n",
    "  if i % target_update == 0:\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "  \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclegan",
   "language": "python",
   "name": "cyclegan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
